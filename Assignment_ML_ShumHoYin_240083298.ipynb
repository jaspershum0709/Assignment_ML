{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkCkZFB-CI1q",
        "outputId": "9f0438c7-62fa-4d74-d08b-464038d5e4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python tqdm psutil scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "TARGET_NEURONS = 13065\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "IMAGE_SIZE = (48, 48)\n",
        "DROPOUT_RATE = 0.5\n",
        "L2_REG = 0.01\n",
        "\n",
        "print(f\"Config: {EPOCHS} epochs, Batch size: {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obc0wrhxCRT_",
        "outputId": "a7e5254a-cafc-4f5f-acf7-083d10e6f8f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Config: 20 epochs, Batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "!git clone -q https://github.com/chenkenanalytic/handwritting_data_all.git\n",
        "!cat /content/handwritting_data_all/all_data.zip* > /content/handwritting_data_all/all_data.zip\n",
        "!unzip -q -O big5 /content/handwritting_data_all/all_data.zip -d \"/content\"\n",
        "\n",
        "print(f\"Dataset downloaded in {time.time() - start_time:.1f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNmPGaozCSNu",
        "outputId": "515a2eae-a316-4b00-b6b0-a08bdf628af6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded in 99.8 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_fast():\n",
        "    dataset_path = \"/content/cleaned_data\"\n",
        "\n",
        "    print(\"Loading dataset structure...\")\n",
        "    all_folders = os.listdir(dataset_path)\n",
        "\n",
        "    char_files = {}\n",
        "    char_count = 0\n",
        "\n",
        "    # Scan folders for target characters\n",
        "    for folder in tqdm(all_folders[:2000]):\n",
        "        folder_path = os.path.join(dataset_path, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        png_files = glob.glob(os.path.join(folder_path, \"*.png\"))\n",
        "        for file_path in png_files:\n",
        "            char_name = os.path.basename(file_path).split('_')[0]\n",
        "            if char_name not in char_files:\n",
        "                if char_count >= TARGET_NEURONS:\n",
        "                    break\n",
        "                char_files[char_name] = []\n",
        "                char_count += 1\n",
        "            char_files[char_name].append(file_path)\n",
        "\n",
        "        if char_count >= TARGET_NEURONS:\n",
        "            break\n",
        "\n",
        "    print(f\"Loaded {len(char_files)} characters\")\n",
        "\n",
        "    # Create character mapping\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(char_files.keys())}\n",
        "\n",
        "    return char_files, char_to_idx\n",
        "\n",
        "char_files, char_map = load_dataset_fast()\n",
        "print(f\"Characters loaded: {len(char_map)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpTEYmikCUGU",
        "outputId": "63dae951-276d-4120-c02e-f63758c83267"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset structure...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 4592.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2000 characters\n",
            "Characters loaded: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_original_data(char_files, char_map, target_size=(48, 48)):\n",
        "    print(\"Loading original dataset...\")\n",
        "\n",
        "    train_images, train_labels = [], []\n",
        "    test_images, test_labels = [], []\n",
        "\n",
        "    for char, files in tqdm(char_files.items()):\n",
        "        files_sorted = sorted(files)\n",
        "\n",
        "        # Training data (first 40 samples)\n",
        "        for file_path in files_sorted[:40]:\n",
        "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, target_size)\n",
        "                img = img.astype('float32') / 255.0\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "                train_images.append(img)\n",
        "                train_labels.append(char_map[char])\n",
        "\n",
        "        # Testing data (remaining samples)\n",
        "        for file_path in files_sorted[40:]:\n",
        "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, target_size)\n",
        "                img = img.astype('float32') / 255.0\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "                test_images.append(img)\n",
        "                test_labels.append(char_map[char])\n",
        "\n",
        "    return (np.array(train_images), np.array(train_labels),\n",
        "            np.array(test_images), np.array(test_labels))\n",
        "\n",
        "X_train_orig, y_train_orig, X_test, y_test = load_original_data(char_files, char_map, IMAGE_SIZE)\n",
        "print(f\"Original training data: {X_train_orig.shape}\")\n",
        "print(f\"Testing data: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsxZhCNjCVVt",
        "outputId": "839a1c28-68b2-484e-d492-1df07180706a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading original dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [02:07<00:00, 15.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training data: (79905, 48, 48, 1)\n",
            "Testing data: (24694, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimized_augment_images(images, labels, target_samples_per_class=100):\n",
        "\n",
        "    print(f\"Starting optimized data augmentation - target: {target_samples_per_class} samples per character\")\n",
        "\n",
        "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='constant',\n",
        "        cval=0\n",
        "    )\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    print(f\"Processing {len(unique_labels)} unique characters...\")\n",
        "\n",
        "    for class_label in tqdm(unique_labels, desc=\"Augmentation\"):\n",
        "        class_mask = (labels == class_label)\n",
        "        class_images = images[class_mask]\n",
        "        original_count = len(class_images)\n",
        "\n",
        "        if original_count == 0:\n",
        "            continue\n",
        "\n",
        "        augmented_images.extend(class_images)\n",
        "        augmented_labels.extend([class_label] * original_count)\n",
        "\n",
        "        needed_augmented = target_samples_per_class - original_count\n",
        "\n",
        "        if needed_augmented > 0:\n",
        "            batch_size_aug = min(needed_augmented, 50)\n",
        "\n",
        "            class_generator = datagen.flow(\n",
        "                class_images,\n",
        "                batch_size=batch_size_aug,\n",
        "                shuffle=False\n",
        "            )\n",
        "\n",
        "            aug_batch = next(class_generator)\n",
        "            num_to_take = min(len(aug_batch), needed_augmented)\n",
        "\n",
        "            augmented_images.extend(aug_batch[:num_to_take])\n",
        "            augmented_labels.extend([class_label] * num_to_take)\n",
        "\n",
        "    X_augmented = np.array(augmented_images)\n",
        "    y_augmented = np.array(augmented_labels)\n",
        "\n",
        "    X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=42)\n",
        "\n",
        "    print(f\"Augmentation completed:\")\n",
        "    print(f\"  Original samples: {len(images)}\")\n",
        "    print(f\"  Augmented samples: {len(X_augmented)}\")\n",
        "    print(f\"  Average per class: {len(X_augmented) // len(unique_labels)}\")\n",
        "\n",
        "    return X_augmented, y_augmented\n",
        "\n",
        "X_train_augmented, y_train_augmented = optimized_augment_images(\n",
        "    X_train_orig, y_train_orig, target_samples_per_class=100\n",
        ")\n",
        "\n",
        "print(f\"Final training data: {X_train_augmented.shape}\")\n",
        "print(f\"Final training labels: {y_train_augmented.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02BoWZMvCYAk",
        "outputId": "53e2f282-836b-4612-ee58-20eb4797c66a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting optimized data augmentation - target: 100 samples per character\n",
            "Processing 2000 unique characters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmentation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:34<00:00, 57.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation completed:\n",
            "  Original samples: 79905\n",
            "  Augmented samples: 159810\n",
            "  Average per class: 79\n",
            "Final training data: (159810, 48, 48, 1)\n",
            "Final training labels: (159810,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=7, min_delta=0.002):\n",
        "        super().__init__()\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_weights = None\n",
        "        self.best_acc = 0\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_acc = logs.get('val_accuracy', 0)\n",
        "\n",
        "        if current_acc > self.best_acc + self.min_delta:\n",
        "            self.best_acc = current_acc\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            self.wait = 0\n",
        "            print(f\"âœ… New best validation accuracy: {current_acc:.4f}\")\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                self.model.set_weights(self.best_weights)\n",
        "                print(f\"ğŸ›‘ Early stopping triggered at epoch {epoch + 1}\")\n",
        "                print(f\"ğŸ¯ Restored best weights, validation accuracy: {self.best_acc:.4f}\")\n",
        "\n",
        "def smoothed_sparse_categorical_crossentropy(y_true, y_pred, smoothing=0.1):\n",
        "    y_true_smooth = y_true * (1 - smoothing) + smoothing / TARGET_NEURONS\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true_smooth, y_pred)"
      ],
      "metadata": {
        "id": "atWeDiL2CcDw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_simple_effective_model1():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (5, 5), activation='relu',\n",
        "                              input_shape=(48, 48, 1)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "        tf.keras.layers.Dense(TARGET_NEURONS, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"Creating Model 1 (Simple & Effective)...\")\n",
        "model1 = create_simple_effective_model1()\n",
        "\n",
        "model1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"Model 1 parameters: {model1.count_params():,}\")\n",
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "oOeLvSqACdeU",
        "outputId": "aa3fc828-6be1-44bb-9d5e-5e947a279016"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Model 1 (Simple & Effective)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 parameters: 1,779,465\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m832\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13065\u001b[0m)          â”‚     \u001b[38;5;34m1,685,385\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13065</span>)          â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,685,385</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,779,465\u001b[0m (6.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,779,465</span> (6.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,779,017\u001b[0m (6.79 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,779,017</span> (6.79 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TRAINING MODEL 1 (Regularized CNN) ===\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "class CleanProgressCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, total_epochs):\n",
        "        self.total_epochs = total_epochs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        try:\n",
        "            lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        except:\n",
        "            lr = 0.001\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{self.total_epochs}: \"\n",
        "              f\"accuracy: {logs['accuracy']:.4f} - loss: {logs['loss']:.4f} - \"\n",
        "              f\"val_accuracy: {logs['val_accuracy']:.4f} - val_loss: {logs['val_loss']:.4f} - \"\n",
        "              f\"learning_rate: {lr:.2e}\")\n",
        "\n",
        "callbacks1 = [\n",
        "    CleanProgressCallback(total_epochs=15),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.00001,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model1.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "try:\n",
        "    history1 = model1.fit(\n",
        "        X_train_augmented, y_train_augmented,\n",
        "        batch_size=64,\n",
        "        epochs=15,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=callbacks1,\n",
        "        verbose=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    model1 = tf.keras.models.load_model('best_model1.keras')\n",
        "    test_loss1, test_accuracy1 = model1.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"Model 1 Test Accuracy: {test_accuracy1:.4f}\")\n",
        "\n",
        "    model1_filename = f'model1_regularized_{int(time.time())}.keras'\n",
        "    model1.save(model1_filename)\n",
        "    print(f\"Model 1 saved as {model1_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model 1 training failed: {e}\")\n",
        "    print(\"Using fallback model...\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APXvJV4rCiC6",
        "outputId": "49221c66-2921-4da6-b776-86740ccf414c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING MODEL 1 (Regularized CNN) ===\n",
            "Epoch 1/15: accuracy: 0.0222 - loss: 6.6287 - val_accuracy: 0.0136 - val_loss: 8.4215 - learning_rate: 1.00e-03\n",
            "Epoch 2/15: accuracy: 0.1649 - loss: 4.3808 - val_accuracy: 0.0532 - val_loss: 7.0002 - learning_rate: 1.00e-03\n",
            "Epoch 3/15: accuracy: 0.3090 - loss: 3.2979 - val_accuracy: 0.0516 - val_loss: 9.5358 - learning_rate: 1.00e-03\n",
            "Epoch 4/15: accuracy: 0.3999 - loss: 2.7521 - val_accuracy: 0.0901 - val_loss: 9.7219 - learning_rate: 1.00e-03\n",
            "Epoch 5/15: accuracy: 0.4544 - loss: 2.4251 - val_accuracy: 0.0488 - val_loss: 12.9421 - learning_rate: 1.00e-03\n",
            "Epoch 6/15: accuracy: 0.4924 - loss: 2.2208 - val_accuracy: 0.2173 - val_loss: 4.2534 - learning_rate: 1.00e-03\n",
            "Epoch 7/15: accuracy: 0.5210 - loss: 2.0607 - val_accuracy: 0.1474 - val_loss: 5.6087 - learning_rate: 1.00e-03\n",
            "Epoch 8/15: accuracy: 0.5386 - loss: 1.9612 - val_accuracy: 0.1773 - val_loss: 7.6869 - learning_rate: 1.00e-03\n",
            "Epoch 9/15: accuracy: 0.5784 - loss: 1.7807 - val_accuracy: 0.3028 - val_loss: 3.9780 - learning_rate: 5.00e-04\n",
            "Epoch 10/15: accuracy: 0.5913 - loss: 1.7236 - val_accuracy: 0.4115 - val_loss: 2.8297 - learning_rate: 5.00e-04\n",
            "Epoch 11/15: accuracy: 0.5986 - loss: 1.6779 - val_accuracy: 0.4718 - val_loss: 3.1375 - learning_rate: 5.00e-04\n",
            "Epoch 12/15: accuracy: 0.6073 - loss: 1.6387 - val_accuracy: 0.6941 - val_loss: 1.3221 - learning_rate: 5.00e-04\n",
            "Epoch 13/15: accuracy: 0.6110 - loss: 1.6095 - val_accuracy: 0.5850 - val_loss: 1.8098 - learning_rate: 5.00e-04\n",
            "Epoch 14/15: accuracy: 0.6165 - loss: 1.5872 - val_accuracy: 0.3869 - val_loss: 3.1411 - learning_rate: 5.00e-04\n",
            "Epoch 15/15: accuracy: 0.6322 - loss: 1.5227 - val_accuracy: 0.6454 - val_loss: 1.5594 - learning_rate: 2.50e-04\n",
            "Model 1 Test Accuracy: 0.6941\n",
            "Model 1 saved as model1_regularized_1763303332.keras\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "907"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_simplified_model2():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Simplified but effective architecture\n",
        "        tf.keras.layers.Conv2D(48, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal',\n",
        "                              input_shape=(48, 48, 1)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(96, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "        tf.keras.layers.Conv2D(192, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        # Direct to output\n",
        "        tf.keras.layers.Dense(TARGET_NEURONS, activation='softmax',\n",
        "                             kernel_initializer='he_normal')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"Creating Model 2 (Simplified Architecture)...\")\n",
        "model2 = create_simplified_model2()\n",
        "\n",
        "optimizer2 = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0003,\n",
        "    clipnorm=1.0\n",
        ")\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=optimizer2,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"Model 2 output neurons: {model2.layers[-1].units}\")\n",
        "print(f\"Model 2 parameters: {model2.count_params():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N43YZELLCpBZ",
        "outputId": "b278cee6-b40a-4472-a6d8-b960815f6593"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Model 2 (Simplified Architecture)...\n",
            "Model 2 output neurons: 13065\n",
            "Model 2 parameters: 2,731,017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TRAINING MODEL 2 (Simplified Architecture) ===\")\n",
        "\n",
        "callbacks2 = [\n",
        "    CleanProgressCallback(total_epochs=12),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.00001,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model2.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Train Model 2\n",
        "    history2 = model2.fit(\n",
        "        X_train_augmented, y_train_augmented,\n",
        "        batch_size=64,\n",
        "        epochs=12,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=callbacks2,\n",
        "        verbose=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    model2 = tf.keras.models.load_model('best_model2.keras')\n",
        "    test_loss2, test_accuracy2 = model2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"Model 2 Test Accuracy: {test_accuracy2:.4f}\")\n",
        "\n",
        "    model2_filename = f'model2_simplified_{int(time.time())}.keras'\n",
        "    model2.save(model2_filename)\n",
        "    print(f\"Model 2 saved as {model2_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model 2 training failed: {e}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VsimPbwCtTP",
        "outputId": "16afbbd1-3bc2-43d1-8221-c222b125ba9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING MODEL 2 (Simplified Architecture) ===\n",
            "Epoch 1/12: accuracy: 0.0021 - loss: 7.7991 - val_accuracy: 0.0111 - val_loss: 6.8045 - learning_rate: 3.00e-04\n",
            "Epoch 2/12: accuracy: 0.0126 - loss: 6.6194 - val_accuracy: 0.0586 - val_loss: 5.8123 - learning_rate: 3.00e-04\n",
            "Epoch 3/12: accuracy: 0.0397 - loss: 5.8798 - val_accuracy: 0.1287 - val_loss: 5.0213 - learning_rate: 3.00e-04\n",
            "Epoch 4/12: accuracy: 0.0822 - loss: 5.2291 - val_accuracy: 0.1319 - val_loss: 4.8247 - learning_rate: 3.00e-04\n",
            "Epoch 5/12: accuracy: 0.1363 - loss: 4.6608 - val_accuracy: 0.3172 - val_loss: 3.6361 - learning_rate: 3.00e-04\n",
            "Epoch 6/12: accuracy: 0.1890 - loss: 4.1819 - val_accuracy: 0.2474 - val_loss: 3.9626 - learning_rate: 3.00e-04\n",
            "Epoch 7/12: accuracy: 0.2459 - loss: 3.7652 - val_accuracy: 0.1885 - val_loss: 5.4815 - learning_rate: 3.00e-04\n",
            "Epoch 8/12: accuracy: 0.2913 - loss: 3.4652 - val_accuracy: 0.5271 - val_loss: 2.3579 - learning_rate: 1.50e-04\n",
            "Epoch 9/12: accuracy: 0.3186 - loss: 3.2980 - val_accuracy: 0.4163 - val_loss: 2.7955 - learning_rate: 1.50e-04\n",
            "Epoch 10/12: accuracy: 0.3437 - loss: 3.1476 - val_accuracy: 0.5375 - val_loss: 2.2006 - learning_rate: 1.50e-04\n",
            "Epoch 11/12: accuracy: 0.3648 - loss: 3.0094 - val_accuracy: 0.6135 - val_loss: 1.8858 - learning_rate: 1.50e-04\n",
            "Epoch 12/12: accuracy: 0.3865 - loss: 2.8871 - val_accuracy: 0.6364 - val_loss: 1.7489 - learning_rate: 1.50e-04\n",
            "Model 2 Test Accuracy: 0.6364\n",
            "Model 2 saved as model2_simplified_1763303669.keras\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1204"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lightweight_model3():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Lightweight but strongly regularized\n",
        "        tf.keras.layers.Conv2D(24, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
        "                              input_shape=(48, 48, 1)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(48, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(L2_REG)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "        tf.keras.layers.Conv2D(96, (3, 3), activation='relu',\n",
        "                              kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        # Minimal classification head\n",
        "        tf.keras.layers.Dense(128, activation='relu',\n",
        "                             kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(TARGET_NEURONS, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"Creating Model 3 (Lightweight)...\")\n",
        "model3 = create_lightweight_model3()\n",
        "\n",
        "optimizer3 = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0004\n",
        ")\n",
        "\n",
        "model3.compile(\n",
        "    optimizer=optimizer3,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"Model 3 output neurons: {model3.layers[-1].units}\")\n",
        "print(f\"Model 3 parameters: {model3.count_params():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xt-njQ4Cuge",
        "outputId": "8b67c114-0ec8-4856-ad7a-3b41ae9e6ea9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Model 3 (Lightweight)...\n",
            "Model 3 output neurons: 13065\n",
            "Model 3 parameters: 1,751,209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Model 3 Training with Clean Output\n",
        "print(\"=== TRAINING MODEL 3 (Lightweight) ===\")\n",
        "\n",
        "callbacks3 = [\n",
        "    CleanProgressCallback(total_epochs=10),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.00001,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model3.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Train Model 3\n",
        "    history3 = model3.fit(\n",
        "        X_train_augmented, y_train_augmented,\n",
        "        batch_size=64,\n",
        "        epochs=10,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=callbacks3,\n",
        "        verbose=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Load best model and evaluate\n",
        "    model3 = tf.keras.models.load_model('best_model3.keras')\n",
        "    test_loss3, test_accuracy3 = model3.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"Model 3 Test Accuracy: {test_accuracy3:.4f}\")\n",
        "\n",
        "    model3_filename = f'model3_lightweight_{int(time.time())}.keras'\n",
        "    model3.save(model3_filename)\n",
        "    print(f\"Model 3 saved as {model3_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model 3 training failed: {e}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev6niRhoCwUJ",
        "outputId": "d606b317-8a07-495c-fb7f-66ef7e455df8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING MODEL 3 (Lightweight) ===\n",
            "Epoch 1/10: accuracy: 0.0038 - loss: 7.6990 - val_accuracy: 0.0046 - val_loss: 7.0860 - learning_rate: 4.00e-04\n",
            "Epoch 2/10: accuracy: 0.0216 - loss: 6.1305 - val_accuracy: 0.0146 - val_loss: 8.1994 - learning_rate: 4.00e-04\n",
            "Epoch 3/10: accuracy: 0.0482 - loss: 5.4746 - val_accuracy: 0.0896 - val_loss: 4.9777 - learning_rate: 4.00e-04\n",
            "Epoch 4/10: accuracy: 0.0744 - loss: 5.0703 - val_accuracy: 0.1639 - val_loss: 4.2820 - learning_rate: 4.00e-04\n",
            "Epoch 5/10: accuracy: 0.0979 - loss: 4.7959 - val_accuracy: 0.0841 - val_loss: 5.8071 - learning_rate: 4.00e-04\n",
            "Epoch 6/10: accuracy: 0.1194 - loss: 4.6033 - val_accuracy: 0.0961 - val_loss: 5.0492 - learning_rate: 4.00e-04\n",
            "Epoch 7/10: accuracy: 0.1437 - loss: 4.3909 - val_accuracy: 0.3768 - val_loss: 3.0694 - learning_rate: 2.00e-04\n",
            "Epoch 8/10: accuracy: 0.1556 - loss: 4.2835 - val_accuracy: 0.1149 - val_loss: 5.5572 - learning_rate: 2.00e-04\n",
            "Epoch 9/10: accuracy: 0.1638 - loss: 4.2121 - val_accuracy: 0.4831 - val_loss: 2.6423 - learning_rate: 2.00e-04\n",
            "Epoch 10/10: accuracy: 0.1727 - loss: 4.1404 - val_accuracy: 0.0107 - val_loss: 21.4263 - learning_rate: 2.00e-04\n",
            "Model 3 Test Accuracy: 0.4831\n",
            "Model 3 saved as model3_lightweight_1763303852.keras\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1328"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== FINAL RESULTS COMPARISON ===\")\n",
        "\n",
        "results = {}\n",
        "model_files = {}\n",
        "\n",
        "try:\n",
        "    if 'test_accuracy1' in locals():\n",
        "        results['Model 1 (Regularized CNN)'] = test_accuracy1\n",
        "        model_files['Model 1 (Regularized CNN)'] = 'best_model1.keras'\n",
        "    if 'test_accuracy2' in locals():\n",
        "        results['Model 2 (Simplified)'] = test_accuracy2\n",
        "        model_files['Model 2 (Simplified)'] = 'best_model2.keras'\n",
        "    if 'test_accuracy3' in locals():\n",
        "        results['Model 3 (Lightweight)'] = test_accuracy3\n",
        "        model_files['Model 3 (Lightweight)'] = 'best_model3.keras'\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if results:\n",
        "    print(\"\\nğŸ“Š Accuracy Results:\")\n",
        "    for name, accuracy in results.items():\n",
        "        print(f\"{name}: {accuracy:.4f}\")\n",
        "\n",
        "    best_model_name = max(results, key=results.get)\n",
        "    best_accuracy = results[best_model_name]\n",
        "\n",
        "    print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
        "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    # Select and save best model\n",
        "    try:\n",
        "        best_model_file = model_files[best_model_name]\n",
        "        best_model = tf.keras.models.load_model(best_model_file)\n",
        "\n",
        "        final_filename = f'best_chinese_character_model_{int(time.time())}.keras'\n",
        "        best_model.save(final_filename)\n",
        "        print(f\"Best model saved as '{final_filename}'\")\n",
        "\n",
        "        # Verify output neurons\n",
        "        final_neurons = best_model.layers[-1].units\n",
        "        print(f\"\\nâœ… Final Verification:\")\n",
        "        print(f\"Output neurons: {final_neurons}\")\n",
        "        print(f\"Required: {TARGET_NEURONS}\")\n",
        "        print(f\"Status: {'âœ… CORRECT' if final_neurons == TARGET_NEURONS else 'âŒ INCORRECT'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving best model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGT70ZInC1pl",
        "outputId": "879e46c5-6423-44ea-e9c6-2e89e8d5dce9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FINAL RESULTS COMPARISON ===\n",
            "\n",
            "ğŸ“Š Accuracy Results:\n",
            "Model 1 (Regularized CNN): 0.6941\n",
            "Model 2 (Simplified): 0.6364\n",
            "Model 3 (Lightweight): 0.4831\n",
            "\n",
            "ğŸ† Best Model: Model 1 (Regularized CNN)\n",
            "Best Accuracy: 0.6941\n",
            "Best model saved as 'best_chinese_character_model_1763303853.keras'\n",
            "\n",
            "âœ… Final Verification:\n",
            "Output neurons: 13065\n",
            "Required: 13065\n",
            "Status: âœ… CORRECT\n"
          ]
        }
      ]
    }
  ]
}